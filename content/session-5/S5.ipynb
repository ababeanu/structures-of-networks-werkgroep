{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "0a9e4012-c6b8-4fce-82cc-cfbbb9363d7c",
      "cell_type": "markdown",
      "source": "# Structures of Networks - workgroup session 5\n\nThe structure of this notebook follows that of the associated problem sheet.\nCode blocks (cells) are already available for (sub)problems that involve coding, but you can always add more.\nSome of these code blocks contain coding-related tips, in the form of comments.",
      "metadata": {}
    },
    {
      "id": "9b4191e3-7c0a-446d-b9eb-b36302b93ad7",
      "cell_type": "markdown",
      "source": "## 1.a.",
      "metadata": {}
    },
    {
      "id": "dc36effb-994c-42a3-b05c-9cb8e5b14f62",
      "cell_type": "code",
      "source": "import networkx as ntx\nimport random as rn\nimport numpy as np\nimport math as mt\nimport matplotlib.pyplot as plt\n\nrn.seed(1)\nnp.random.seed(1)\n\n# randomly generates a list of feature vectors (LFV), containing n vectors of F entries each, where each entry is a binary random (unbiased) variable\ndef random_LFV(n, F):\n    \n    x = []    \n    for i in range(0, n):\n        xi = []\n        for f in range(0, F):\n            xi.append(rn.randint(0, 1))\n        x.append(xi)\n        \n    return x\n\n# computes the (Hamming) similarity between two (binary) vectors of equal lenghts\ndef similarity(x1, x2):\n    F = len(x1)\n    nmatch = 0.0\n    for i in range(0, F):\n        nmatch += int(x1[i] == x2[i])\n    return float(nmatch)/F\n\n# computes the lmbd-adjusted (Hamming similarity between two (binary) vectors of equal lengths\ndef adjusted_similarity(x1, x2, lmbd1, lmbd2):\n    return (similarity(x1, x2))**(lmbd1*lmbd2)\n    \n\n# generates a random graph from a list of feature vectors x, using a lmbd-power-adjusted Hamming similarity, which is interpreted as a link probability\n# here, lmdb (lambda) is a list of positive-real values, one for each of the feature vectors in x (and thus for each of the nodes in the graph being generated) \ndef LFV_graph(x, lmbd):\n    n = len(x)\n    G = ntx.empty_graph(n)\n    for i in range(0, n-1):\n        for j in range(i+1, n):\n            # probability of generating link (i,j):\n            p = adjusted_similarity(x[i], x[j], lmbd[i], lmbd[j])\n            r = rn.random()\n            if(r < p):\n                ntx.add_path(G, [i,j])\n    return G\n\n# computes the the mean of the lmbd-adjusted (Hamming) similarities between all pairs of feature vectors in x\n# this is equivalent to the expected network density, given x and lmbd\ndef mean_adjusted_similarity(x, lmbd):\n    sum = 0.0\n    n = len(x)\n    for i in range(0, n-1):\n        for j in range(i+1, n):\n            sum += adjusted_similarity(x[i], x[j], lmbd[i], lmbd[j])\n    return sum*2.0/n/(n-1)\n\n# computes the expectation value of the degree of node i, given x and lmbd\ndef expected_degree(x, lmbd, i):\n    sum = 0.0\n    for j in range(0, len(x)):\n        if(j != i):\n            sum += adjusted_similarity(x[i], x[j], lmbd[i], lmbd[j])\n    return sum\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "13631d9d-15d3-4f34-9af5-87af2234d17d",
      "cell_type": "markdown",
      "source": "## 1.b.",
      "metadata": {}
    },
    {
      "id": "31a04fb5-3f5f-478a-8d7e-baf23703fc0e",
      "cell_type": "markdown",
      "source": "## 1.c.",
      "metadata": {}
    },
    {
      "id": "0b1d8f25-9522-4c97-8d2f-cb6560540912",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "38a6d132-17fd-4a42-b138-796d21645fae",
      "cell_type": "markdown",
      "source": "## 1.d.",
      "metadata": {}
    },
    {
      "id": "6d88813b-d823-4aa7-9fd5-203441aa6b8f",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f32bdfcf-4aff-4228-9299-684f86c89065",
      "cell_type": "markdown",
      "source": "## 1.e.",
      "metadata": {}
    },
    {
      "id": "c6a10713-9eb5-4fc4-953a-d469efbfa3ae",
      "cell_type": "markdown",
      "source": "## 2.a.",
      "metadata": {}
    },
    {
      "id": "b0cc7ebb-a3da-44f7-b100-c26812137f9a",
      "cell_type": "code",
      "source": "# computes area under the line segment between points c1, c2, where each point is encoded as a 2D coordinate pair (tuple)\ndef slice_area(c1, c2):\n    (x1, y1) = c1\n    (x2, y2) = c2\n    return (y1+y2)*(x2-x1)/2\n\n# computes area under a sequence of line segments defined by coords, which is a list of coordinate pairs\n# the l'th segment goes between coordinate pairs c[l] and c[l+1]\ndef combined_area(coords):\n    A = 0.0\n    for l in range(0,len(coords)-1):\n        A += slice_area(coords[l], coords[l+1])\n    return A\n\n# adjusts the y-components of all coordinate pairs in coords so that the latter can be interprted as a (properly normalized) probability density function with a piecewise-linear shape\n# the coordinates have to be sorted so that the x-components are strictly increasing\ndef normalize(coords):\n    A = combined_area(coords)\n    for l in range(0, len(coords)):\n        (x,y) = coords[l]\n        coords[l] = (float(x), float(y)/A)\n    return coords\n\n# generates a random number sampled from a distribution with a density function that takes a piecewise-linear shape specified by coordinate (x,y) pairs in coords list\n# each y-entry corresponds to the probability density for the respective x-entry\n# the l'th segment of the density function goes between coordinate pairs c[l] and c[l+1]\n# the coordinates have to be sorted so that the x-components are strictly increasing\n# the y-coordinates are automatically rescaled, by the same factor, in order to ensure that the distribution is normalized  \ndef rand_piecewise(coords):\n\n    coords = normalize(coords)\n    \n    r = rn.random()\n    rr = -1.0\n    \n    searching = True\n    l = 0\n    \n    while(searching):\n        A = slice_area(coords[l], coords[l+1])\n\n        if(r <= A):\n            searching = False\n            (x1, y1) = coords[l]\n            (x2, y2) = coords[l+1]\n            a = (y2-y1)/(x2-x1)\n            b = (x2*y1 - x1*y2)/(x2-x1)\n            if(a == 0.0):\n                rr = x1 + r/y1\n            else:\n                aa = a\n                bb = (b - a*x1 + y1)\n                cc = - (x1*(b+y1) + 2*r)\n                rr = (- bb + (bb**2 - 4*aa*cc)**(0.5))/(2.0*aa)\n        else:\n            r -= A\n\n        l += 1\n        \n        if(l == len(coords)-1):\n            searching = False\n\n    return rr\n        ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "046495ce-13d0-4990-93dd-a673f8354300",
      "cell_type": "markdown",
      "source": "## 2.b.",
      "metadata": {}
    },
    {
      "id": "3ad30300-ab62-425c-84a3-ef453c02e349",
      "cell_type": "code",
      "source": "# probability density function (not normalized) consisting of two segments\n#coords = [(0.0, 0.0), (1.0, 2.0), (2.0, 1.0)]\n\n\n# normalizing and separating the x and y coordinates in two associated lists\n#cx = []\n#cy = []\n#for c in normalize(coords):\n#    (x, y) = c\n#    cx.append(x)\n#    cy.append(y)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "32688bb6-1ab4-41dd-a837-1169bc899139",
      "cell_type": "markdown",
      "source": "## 2.c.",
      "metadata": {}
    },
    {
      "id": "441e0bcb-734b-4902-81a7-e2982534df74",
      "cell_type": "code",
      "source": "#plt.hist(degrees)\n#plt.xscale('log')\n#plt.yscale('log')\n#plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0e9f7c1-af2d-4e9b-97d8-426531bb0fc6",
      "cell_type": "markdown",
      "source": "## 3.a.",
      "metadata": {}
    },
    {
      "id": "56914bb8-34d2-4914-b477-e41dab5abe53",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "24bf898e-c399-40b9-9edc-6c339c37bdcf",
      "cell_type": "markdown",
      "source": "## 3.b.",
      "metadata": {}
    },
    {
      "id": "f439c483-e22c-49a8-9e71-a035876a0738",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}