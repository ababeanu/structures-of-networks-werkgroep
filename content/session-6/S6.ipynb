{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "0a9e4012-c6b8-4fce-82cc-cfbbb9363d7c",
      "cell_type": "markdown",
      "source": "# Structures of Networks - workgroup session 6\n\nThe structure of this notebook follows that of the associated problem sheet.\nCode blocks (cells) are already available for (sub)problems that involve coding, but you can always add more.\nSome of these code blocks contain coding-related tips, in the form of comments.",
      "metadata": {}
    },
    {
      "id": "9b4191e3-7c0a-446d-b9eb-b36302b93ad7",
      "cell_type": "markdown",
      "source": "## 1.a.",
      "metadata": {}
    },
    {
      "id": "a199e99c-aa3f-409a-929b-046ac17d2ce2",
      "cell_type": "code",
      "source": "import networkx as ntx\nimport random as rn\nimport numpy as np\nimport math as mt\nimport matplotlib.pyplot as plt\n\nrn.seed(1)\nnp.random.seed(1)\n\n# randomly generates a list of feature vectors (LFV), containing n vectors of F entries each, where each entry is a binary random (unbiased) variable\ndef random_LFV(n, F):\n    x = []    \n    for i in range(0, n):\n        xi = []\n        for f in range(0, F):\n            xi.append(rn.randint(0, 1))\n        x.append(xi)\n        \n    return x\n\n# randomly generates a list of feature vectors with categories (LFVC), containing n vectors of F entries (feature/variable values)\n# the number of values that each feature may take matches the number of categories, specified by argument C\n# each category has un underlying, latent feature vector, populated by one value, constant across features and distinct from the (constant) values associated to other latent vectors\n# each vector is randomly assigned to one category and generated as a partial copy of that category's latent vector\n# for each vector, the values of D randomly selected features are copied from its category's latent vector, while the other are generated randomly\n# returns the list of generated feature vectors, along with a list of associated integers encoding the ground-truth partition - each entry identifies the a-priori category of the vector\ndef random_LFVC(n, F, C, D):\n    # list of feature vectors\n    x = []\n    # ground truth partition, containing one integer for each feature vector\n    gtp = [] \n\n    for i in range(0,n):\n        xi = []\n        # picking category\n        c = rn.choice(range(0,C))\n        gtp.append(c)\n        # sampling without replacement, using rn.sample(), D features that are to be copied from the category's latent vector\n        lf = sorted(rn.sample(range(0,F), D))\n        for a in range(0,F):\n            genrand = False\n            if(len(lf) > 0):\n                if(a == lf[0]):\n                    xi.append(c)\n                    del lf[0]\n                else:\n                    genrand = True\n            else:\n                genrand = True\n            if(genrand):\n                xi.append(rn.choice(range(0,C)))\n        x.append(xi)\n    \n    return x, gtp\n\n# applies a joint sorting operation on data compliant with the output of random_LFVC function above\n# carries out sorting for increasing values of the ground truth partition (gtp) list\n# returns the consistently sorted counterparts of the set of feature vectors (x) and ground truth partition (gtp), along with the list of vector ids (consistent with the initial ordering)\ndef joint_sort(x, gtp):\n\n    ids = range(0,n)\n\n    # assembling list of tuples\n    lt = []\n    for i in range(0,n):\n        lt.append((x[i], gtp[i], ids[i]))\n\n    # sorting the list of tuples according to gtp value\n    lt_s = sorted(lt, key=lambda t: t[1])  \n\n    # unpacking the sorted list of tuples\n    x_s = []\n    gtp_s = []\n    ids_s = []\n    for i in range(0,n):\n        (x_i, gtp_i, ids_i) = lt_s[i]\n        x_s.append(x_i)\n        gtp_s.append(gtp_i)\n        ids_s.append(ids_i)\n    \n    return x_s, gtp_s, ids_s\n\n# computes the (Hamming) similarity between two (binary) vectors of equal lenghts\ndef similarity(x1, x2):\n    F = len(x1)\n    nmatch = 0.0\n    for a in range(0, F):\n        nmatch += int(x1[a] == x2[a])\n    return float(nmatch)/F\n\n# computes the lmbd-adjusted (Hamming similarity between two (binary) vectors of equal lengths\ndef adjusted_similarity(x1, x2, lmbd1, lmbd2):\n    return (similarity(x1, x2))**(lmbd1*lmbd2)\n    \n\n# generates a random graph from a list of feature vectors x, using a lmbd-power-adjusted Hamming similarity, which is interpreted as a link probability\n# here, lmdb (lambda) is a list of positive-real values, one for each of the feature vectors in x (and thus for each of the nodes in the graph being generated) \ndef LFV_graph(x, lmbd):\n    n = len(x)\n    G = ntx.empty_graph(n)\n    for i in range(0, n-1):\n        for j in range(i+1, n):\n            # probability of generating link (i,j):\n            p = adjusted_similarity(x[i], x[j], lmbd[i], lmbd[j])\n            r = rn.random()\n            if(r < p):\n                ntx.add_path(G, [i,j])\n    return G\n\n# computes the the mean of the lmbd-adjusted (Hamming) similarities between all pairs of feature vectors in x\n# this is equivalent to the expected network density, given x and lmbd\ndef mean_adjusted_similarity(x, lmbd):\n    sum = 0.0\n    n = len(x)\n    for i in range(0, n-1):\n        for j in range(i+1, n):\n            sum += adjusted_similarity(x[i], x[j], lmbd[i], lmbd[j])\n    return sum*2.0/n/(n-1)\n\n# computes the expectation value of the degree of node i, given x and lmbd\ndef expected_degree(x, lmbd, i):\n    sum = 0.0\n    for j in range(0, len(x)):\n        if(j != i):\n            sum += adjusted_similarity(x[i], x[j], lmbd[i], lmbd[j])\n    return sum\n\n# computes area under the line segment between points c1, c2, where each point is encoded as a 2D coordinate pair (tuple)\ndef slice_area(coord1, coord2):\n    (x1, y1) = coord1\n    (x2, y2) = coord2\n    return (y1+y2)*(x2-x1)/2\n\n# computes area under a sequence of line segments defined by coords, which is a list of coordinate pairs\n# the l'th segment goes between coordinate pairs c[l] and c[l+1]\ndef combined_area(coords):\n    A = 0.0\n    for l in range(0,len(coords)-1):\n        A += slice_area(coords[l], coords[l+1])\n    return A\n\n# adjusts the y-components of all coordinate pairs in coords so that the latter can be interprted as a (properly normalized) probability density function with a piecewise-linear shape\n# the coordinates have to be sorted so that the x-components are strictly increasing\ndef normalize(coords):\n    A = combined_area(coords)\n    for l in range(0, len(coords)):\n        (x,y) = coords[l]\n        coords[l] = (float(x), float(y)/A)\n    return coords\n\n# generates a random number sampled from a distribution with a density function that takes a piecewise-linear shape specified by coordinate (x,y) pairs in coords list\n# each y-entry corresponds to the probability density for the respective x-entry\n# the l'th segment of the density function goes between coordinate pairs c[l] and c[l+1]\n# the coordinates have to be sorted so that the x-components are strictly increasing\n# the y-coordinates are automatically rescaled, by the same factor, in order to ensure that the distribution is normalized  \ndef rand_piecewise(coords):\n\n    coords = normalize(coords)\n    \n    r = rn.random()\n    rr = -1.0\n    \n    searching = True\n    l = 0\n    \n    while(searching):\n        A = slice_area(coords[l], coords[l+1])\n\n        if(r <= A):\n            searching = False\n            (x1, y1) = coords[l]\n            (x2, y2) = coords[l+1]\n            a = (y2-y1)/(x2-x1)\n            b = (x2*y1 - x1*y2)/(x2-x1)\n            if(a == 0.0):\n                rr = x1 + r/y1\n            else:\n                aa = a\n                bb = (b - a*x1 + y1)\n                cc = - (x1*(b+y1) + 2*r)\n                rr = (- bb + (bb**2 - 4*aa*cc)**(0.5))/(2.0*aa)\n        else:\n            r -= A\n\n        l += 1\n        \n        if(l == len(coords)-1):\n            searching = False\n\n    return rr\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "09eec76a-b7de-43fc-b3f5-93b808c7717d",
      "cell_type": "markdown",
      "source": "## 1.b",
      "metadata": {}
    },
    {
      "id": "ba2e02f5-1074-47a9-9544-00a31a9e4c5b",
      "cell_type": "code",
      "source": "# specify parameters\n\n# invoke generation function\n\n# invoke sorting function\n\n# if ids_s is the sorted list of feature vector ids, then the following code gives you the permutation used during the sorting:\n# perm = [0]*n\n# for i in range(0,n):\n#    perm[ids_s[i]] = i",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "29100bd7-6e78-4262-bff9-ef3b19c07d80",
      "cell_type": "code",
      "source": "# compute the (sorted) similarity matrix s_s",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "55808d92-ea4e-42bd-bec9-4c105bdb8df9",
      "cell_type": "code",
      "source": "# visualize the (sorted) similarity matrix s_s using:\n# plt.matshow(s_s)\n# plt.colorbar()\n# plt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0af3efc7-5822-4117-b620-07c821146272",
      "cell_type": "markdown",
      "source": "## 1.c.",
      "metadata": {}
    },
    {
      "id": "4b15e802-d110-4597-9c53-f542d3993dd8",
      "cell_type": "code",
      "source": "# specify the lambda distribution via a set of coordinates pairs like:\n# lmbd_pdf_coords = [(0.5, 0.0), (1.0, 1.0), (1.5, 0.0)]\n\n# then generate a vector of lambda values using something like:\n# lmbd = []\n# for i in range(0,n):\n#     lmbd.append(rand_piecewise(lmbd_pdf_coords))\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "64bea604-da1d-4e3e-870f-73541b4a46dc",
      "cell_type": "code",
      "source": "# visualize the sampled lambda values using a histogram",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c389e9f7-5008-46bd-9eb0-bd7061761034",
      "cell_type": "code",
      "source": "# compute the (sorted) adjusted similarity matrix sa_s like in 1.b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e916d1fb-1249-4e05-b35d-bebe14a1e44c",
      "cell_type": "code",
      "source": "# visualize the (sorted) adusted similarity matrix sa_s like in 1.b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "438e1ecf-7f1c-4335-9762-bfbae369e1e3",
      "cell_type": "markdown",
      "source": "## 1.d.",
      "metadata": {}
    },
    {
      "id": "99f9dd7f-9ecb-4de6-b79f-2f79acfbe4c8",
      "cell_type": "code",
      "source": "# generate a random network FDRG",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e65b4e95-d94e-48a4-97f2-e7764541cee4",
      "cell_type": "code",
      "source": "# initialize a (sorted) adjacency matrix filled it with zeroes:\n# a_s = np.empty((n,n))\n# for i in range(0,n):\n#     for j in range(0,n):\n#         a_s[i][j] = 0.0\n# then set to 1.0 the entries for which there is a link in FDRG, while accounting for the sorting carried out in 1.b.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "12c448d2-d526-4ed2-af0a-831cc6889be5",
      "cell_type": "code",
      "source": "# visualize the (sorted) adjacency matrix a_s like in 1.b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0c3dc118-6e46-43c5-860f-41d729133be3",
      "cell_type": "markdown",
      "source": "## 1.e.",
      "metadata": {}
    },
    {
      "id": "282f7147-e405-4f8c-bbdc-8144d2f81f55",
      "cell_type": "code",
      "source": "# run the Louvain method on the FDRG network generated above",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b8d0ca01-aa55-4ce9-9ef8-3f1a18bafaf4",
      "cell_type": "code",
      "source": "# convert the output to the format inherent to the ground-truth partition generated in 1.b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e512b0bd-cf22-405d-9b66-4959e3a763a8",
      "cell_type": "markdown",
      "source": "## 1.f.",
      "metadata": {}
    },
    {
      "id": "3af4cf30-6541-4db1-ba11-b38002d63934",
      "cell_type": "code",
      "source": "# generate the joint distribution matrix",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1342b277-6adb-4885-b975-76eb58d5462c",
      "cell_type": "code",
      "source": "# visualize the joint distribution matrix like in 1.b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "638177c0-7f31-41d7-9be3-d5bca0e86a74",
      "cell_type": "markdown",
      "source": "## 1.g",
      "metadata": {}
    },
    {
      "id": "641cf900-2108-45ee-83bb-e863f430eaf4",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}